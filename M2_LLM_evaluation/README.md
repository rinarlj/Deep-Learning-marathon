# LLM Evaluation on MMLU  

This project evaluates **large language models (LLMs)** on the **Measuring Massive Multitask Language Understanding (MMLU)** benchmark.  

## Models Evaluated  
- GPT-2  
- Flan-T5-Large  
- LLaMA-3.1-8B  
- Mistral-7B  
- Gemma-2-2B  

## Objective  
Compare model performance across multiple knowledge domains using MMLU.  